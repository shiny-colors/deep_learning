{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c284067250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from utils import Vocab\n",
    "from utils.torch import DataLoader\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    '''\n",
    "    早期終了 (early stopping)\n",
    "    '''\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False\n",
    "    \n",
    "def sort(x, t):\n",
    "    lens = [len(i) for i in x]\n",
    "    indices = sorted(range(len(lens)), key=lambda i: -lens[i])\n",
    "    x = [x[i] for i in indices]\n",
    "    t = [t[i] for i in indices]\n",
    "    return (x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "data_dir = os.getcwd() + \"\\\\data\\\\small_parallel_enja-master\"\n",
    "en_train_path = os.path.join(data_dir, \"train.en\")\n",
    "en_val_path = os.path.join(data_dir, \"dev.en\")\n",
    "en_test_path = os.path.join(data_dir, \"test.en\")\n",
    "\n",
    "ja_train_path = os.path.join(data_dir, \"train.ja\")\n",
    "ja_val_path = os.path.join(data_dir, \"dev.ja\")\n",
    "ja_test_path = os.path.join(data_dir, \"test.ja\")\n",
    "\n",
    "en_vocab = Vocab()\n",
    "ja_vocab = Vocab()\n",
    "\n",
    "en_vocab.fit(en_train_path, encoding=\"utf-8\")\n",
    "ja_vocab.fit(ja_train_path, encoding=\"utf-8\")\n",
    "\n",
    "x_train = en_vocab.transform(en_train_path, encoding=\"utf-8\")\n",
    "x_val = en_vocab.transform(en_val_path, encoding=\"utf-8\")\n",
    "x_test = en_vocab.transform(en_test_path, encoding=\"utf-8\")\n",
    "\n",
    "t_train = ja_vocab.transform(ja_train_path, eos=True, encoding=\"utf-8\")\n",
    "t_val = ja_vocab.transform(ja_val_path, eos=True, encoding=\"utf-8\")\n",
    "t_test = ja_vocab.transform(ja_test_path, eos=True, encoding=\"utf-8\")\n",
    "\n",
    "(x_train, t_train) = sort(x_train, t_train)\n",
    "(x_val, t_val) = sort(x_val, t_val)\n",
    "(x_test, t_test) = sort(x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの作成\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataloader = DataLoader((x_train, t_train), batch_first=False, device=device)\n",
    "val_dataloader = DataLoader((x_val, t_val), batch_first=False, device=device)\n",
    "test_dataloader = DataLoader((x_test, t_test), batch_first=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "# Encoder層を定義\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, maxlen=20, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device =device\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        len_source_sequences = (x.T > 0).sum(dim=-1)\n",
    "        x = self.embedding(x)\n",
    "        x = pack_padded_sequence(x, len_source_sequences)\n",
    "        h, states = self.lstm(x)\n",
    "        h, _ = pad_packed_sequence(h)\n",
    "        return h, states\n",
    "    \n",
    "# Decoder層を定義\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.attn = Attention(hidden_dim, hidden_dim, device=self.device)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(self.out.weight)\n",
    "\n",
    "    def forward(self, x, hs, states, source=None):\n",
    "        x = self.embedding(x)\n",
    "        ht, states = self.lstm(x, states)\n",
    "        ht = self.attn(ht, hs, source=source)\n",
    "        y = self.out(ht)\n",
    "        return y, states\n",
    "    \n",
    "# Attention層を定義\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.W_a = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.W_c = nn.Parameter(torch.Tensor(hidden_dim + hidden_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "        nn.init.xavier_normal_(self.W_a)\n",
    "        nn.init.xavier_normal_(self.W_c)\n",
    "\n",
    "    def forward(self, ht, hs, source=None):\n",
    "        # スコア関数の計算\n",
    "        score = torch.einsum(\"jik, kl->jil\", (hs, self.W_a))\n",
    "        score = torch.einsum(\"jik, lik->jil\", (ht, score))\n",
    "\n",
    "        # スコア関数を正規化\n",
    "        score = score - torch.max(score, dim=-1, keepdim=True)[0]\n",
    "        score = torch.exp(score)\n",
    "        if source is not None:\n",
    "            mask_source = source.t().eq(0).unsqueeze(0)   # バンディング部分を求める\n",
    "            score.data.masked_fill_(mask_source, 0)   #マスク処理\n",
    "        a = score / torch.sum(score, dim=-1, keepdim=True)\n",
    "\n",
    "        # 文脈ベクトルの計算\n",
    "        c = torch.einsum(\"jik, kil->jil\", (a, hs))\n",
    "\n",
    "        # 出力の計算\n",
    "        h = torch.cat((c, ht), -1)\n",
    "        return torch.tanh(torch.einsum(\"jik, kl->jil\", (h, self.W_c)) + self.b)\n",
    "    \n",
    "# Encoder Decoderモデルを定義\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, maxlen=20, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, device=device)\n",
    "        self.decoder = Decoder(hidden_dim, output_dim, device=device)\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, source, target=None, use_teacher_forcing=False):\n",
    "        batch_size = source.size(1)\n",
    "        if target is not None:\n",
    "            len_target_sequences = target.size(0)\n",
    "        else:\n",
    "            len_target_sequences = self.maxlen\n",
    "\n",
    "        hs, states = self.encoder(source)\n",
    "        y = torch.ones((1, batch_size), dtype=torch.long, device=self.device)\n",
    "        output = torch.zeros((len_target_sequences, batch_size, self.output_dim), device=self.device)\n",
    "\n",
    "        for t in range(len_target_sequences):\n",
    "            out, states = self.decoder(y, hs, states, source=source)\n",
    "            output[t] = out\n",
    "\n",
    "            if use_teacher_forcing and target is not None:\n",
    "                y = target[t].unsqueeze(0)        \n",
    "            else:\n",
    "                y = out.max(-1)[1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 100])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = x.T.reshape(-1).tolist()\n",
    "target = t.T.reshape(-1).tolist()\n",
    "out = preds.max(dim=-1)[1].T.reshape(-1).tolist()\n",
    "\n",
    "source = ' '.join(en_vocab.decode(source))\n",
    "target = ' '.join(ja_vocab.decode(target))\n",
    "out = ' '.join(ja_vocab.decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = x[:, 0].tolist()\n",
    "target = t[:, 0].T.tolist()\n",
    "out = preds.max(dim=-1)[1].T.reshape(-1).tolist()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2597, 6376, 5158, 6160, 3455, 2130, 5749, 4704,  385,   94],\n",
       "        [3448, 4487, 1757, 6053, 4730, 1968,  725, 5373, 6582,   94],\n",
       "        [1479, 2392, 4704, 5160, 5271,  457, 5235,  909, 2144,   94],\n",
       "        [4246, 5564, 6127, 1968, 1721, 6002, 6037, 4704, 3932, 1324],\n",
       "        [1515,  448, 2224,  125, 4755, 1137, 4248, 3431, 3365,   94],\n",
       "        [3448, 5660, 6037, 1522,  398, 2597, 4286, 3843, 1546,   94],\n",
       "        [1546,  684, 5749, 4704,  385, 5660, 4463, 6488,  149,   94],\n",
       "        [1845, 5150,  398, 1845, 5925, 6287, 5133, 4704, 4320,   94],\n",
       "        [2597, 2276, 1481, 5564, 5773, 3683, 3455, 2649, 6149,   94],\n",
       "        [2597, 3700, 6261, 6417, 2597, 2863, 6287, 4815, 5564,   94],\n",
       "        [1479, 5660, 4904, 5267, 1968, 3413, 4730, 4704, 5235,   94],\n",
       "        [2863, 5564, 2696, 6053, 6190, 4704, 1258, 5564, 2897, 1324],\n",
       "        [1845, 2249, 4666, 2224, 6582, 1515, 3455, 2997, 3875,   94],\n",
       "        [1515, 4487, 3326, 6417, 4248, 4704, 4670, 4487, 2515,   94],\n",
       "        [2597, 2392, 3894, 2515, 3823, 3455, 2734, 6037, 1522,   94],\n",
       "        [2597, 2392, 3407, 6417,  220, 2022, 4730, 4704, 5717,   94],\n",
       "        [3584, 6376, 2597, 5495,    9, 3448, 4127, 1968, 4938,   94],\n",
       "        [2597, 3700, 6261,   94, 2597, 1721, 2883, 2649, 2301,   94],\n",
       "        [1479, 2952, 3972, 1984, 6053, 4704,  937,   53, 3875,   94],\n",
       "        [3448, 4487,  383,  398, 1479, 1633, 1263, 1968, 1334,   94],\n",
       "        [2827, 5499, 4704, 3759, 1821, 3352, 6037, 4704, 4318, 1324],\n",
       "        [3691, 5082, 2597, 1432, 4704, 6315, 5564, 2650, 5659,   94],\n",
       "        [2827, 5082, 5564, 1450, 1968, 6127, 6053, 5158, 1563, 1324],\n",
       "        [3448, 4487, 1944, 3680, 3336, 1968, 3625, 6350, 1463,   94],\n",
       "        [4704, 4250, 6350, 1463, 4487,  398, 1479, 4487, 1808,   94],\n",
       "        [4704, 3564, 5461, 1137, 6037, 3430, 3455, 4704, 2758,   94],\n",
       "        [1479, 4792, 5804, 6037, 4704,  375, 3455,  457, 5518,   94],\n",
       "        [3117, 3383, 4207, 4704,  961, 1843, 3455, 5373,  912,   94],\n",
       "        [1479, 5660, 4704, 4286,  881, 2597,  970, 1968, 1481,   94],\n",
       "        [6149, 2141, 3455, 4543, 3972, 4293, 4295, 4286, 2358,   94],\n",
       "        [2597, 2863, 6287, 3120, 6350, 5564, 6037, 5373,  521,   94],\n",
       "        [ 125, 2650, 1051, 2120, 1968, 4652, 3455,  457, 4213,   94],\n",
       "        [2649, 3443, 4487, 1051,  961, 5749, 6081, 4704, 3717,   94],\n",
       "        [2827, 3681, 5564, 5082, 4522, 5564, 3843, 5158,  230, 1324],\n",
       "        [4704, 1474, 6037, 5373, 4034, 4246, 1757, 1968, 3348,   94],\n",
       "        [3448, 4487, 5200, 6053, 4730, 1968, 1370, 4704, 2341,   94],\n",
       "        [2597, 2249, 1263, 6417, 2879, 4704, 2232, 4487,  729,   94],\n",
       "        [2597, 2054, 6334, 6037, 4704, 2765, 5271, 5373, 1761,   94],\n",
       "        [2273, 3455, 4704, 1184, 3214, 5082, 5564, 1547,  710, 1324],\n",
       "        [3448, 4487, 1757, 6053, 5564, 1968, 1370, 5373, 2341,   94],\n",
       "        [3200, 4506, 4487, 1051, 5370, 6417, 2993, 6287, 1845, 1324],\n",
       "        [4704, 3326,  912, 1850, 6037, 5158, 6160, 3455, 1258,   94],\n",
       "        [4178, 1263, 1968, 3490, 1695, 4178, 1446, 1968,  601,   94],\n",
       "        [3448, 4727, 4730, 4881, 4900, 1968, 5082, 2649, 3356,   94],\n",
       "        [4178, 6534, 1968, 3061, 4704,  449, 3455, 4704, 4793,   94],\n",
       "        [1845, 4587, 1968,  218,  863, 5158, 5630, 1968,  373,   94],\n",
       "        [1479, 5879, 3448, 5749, 4704, 4235, 3455,  457, 5590,   94],\n",
       "        [5564, 2249, 2874, 5052, 5303, 1968, 4704, 3326, 5930,   94],\n",
       "        [1479, 4487, 5494,  773,  711, 1479, 4496, 1968,  510,   94],\n",
       "        [ 220, 4487, 3448,  398, 5564, 1446, 1968, 5165, 1968, 1324],\n",
       "        [2249, 5564, 3404, 4730, 5158, 2191, 1968, 2649, 2085, 1324],\n",
       "        [3448, 4487, 5208, 2073, 4295, 2597, 3843, 5564, 4286,   94],\n",
       "        [5373, 6582, 3349, 2137, 6287,  510, 6190,  398, 2159,   94],\n",
       "        [ 218, 5564, 5927, 4496, 1968, 1082, 6541, 4688, 3764, 1324],\n",
       "        [4178, 5870, 4704, 6222, 1577, 6417, 6118, 6287, 4178, 1324],\n",
       "        [1479, 3395, 4730, 1944, 5494,  711, 5208, 4710, 1452,   94],\n",
       "        [2597, 6397, 4704, 1132, 5499, 6127, 1968, 4690, 4497,  114],\n",
       "        [6067, 4246, 5158, 2698, 2617, 2047, 6037, 5373, 3021,   94],\n",
       "        [3192, 2804, 2647,    8,    9, 5564,    5, 4543, 2130,   94],\n",
       "        [5373,  428, 6067, 2650,   53, 4723, 3875, 6037,   10,   94],\n",
       "        [4487, 6067, 6390,  861, 5564, 3681, 1547, 1968,  647, 1324],\n",
       "        [2649, 3443, 5660, 1051, 4533, 6350, 4704, 5588, 3846,   94],\n",
       "        [3448, 5499, 2212, 3192, 4704,  387, 3431, 1968, 1012,   94],\n",
       "        [3448, 4487, 3192, 6053, 5564, 1968, 1263, 1968, 1012,   94],\n",
       "        [ 457, 2445, 2022, 1463, 1968, 4690, 4877, 5158, 1978,   94],\n",
       "        [3448, 5660, 2893, 1569, 2597,  725, 6037, 5158, 2894,   94],\n",
       "        [2597, 5660, 4225, 5749, 3150,  803, 4704, 5282, 2650,   94],\n",
       "        [1479, 2952, 5158, 4061, 2273, 4487, 1757, 1968, 3348,   94],\n",
       "        [1479, 5100, 5158, 5518, 6037, 3430, 3455, 2617, 3855,   94],\n",
       "        [2656, 2952, 6242, 3764, 3972, 5848, 6053, 5158, 4320,   94],\n",
       "        [5879, 5564, 2316, 6390, 2597, 2863, 2258, 5271, 6458, 1324],\n",
       "        [2597, 1533,  909, 4157, 2774, 5173, 3513, 1824, 4312,   94],\n",
       "        [1479, 5692,  457, 2515, 3564, 6053, 5158, 3326, 1515,   94],\n",
       "        [1479, 2952,  253, 6417,  334, 1479, 2952, 2617,  261,   94],\n",
       "        [3448, 4487, 2893,  398, 4178, 1633, 6476,  457, 1403,   94],\n",
       "        [2597,  218,  744, 4130, 5106, 2617, 2047, 5106, 1479,   94],\n",
       "        [2649, 6149,   46, 6141, 6037, 5158, 1051,   80,  287,   94],\n",
       "        [5373, 4487, 5208, 4710, 1452, 6037, 4704, 6541, 1452,   94],\n",
       "        [1479, 5268,  448,  883, 3265, 1657, 4178, 2650, 3365,   94],\n",
       "        [4178, 6024, 1944, 3192, 1525, 3365, 3455, 4704, 2440,   94],\n",
       "        [4704, 3962, 4510, 6053, 4704, 3247, 6024,  457, 1884,   94],\n",
       "        [4178, 2249, 6190, 4094, 2647, 5564, 1968, 4704, 4286,   94],\n",
       "        [1479, 5660, 5749, 5158, 1018, 2273, 5235, 1968, 1721,   94],\n",
       "        [1479, 5499, 6242,  402, 6417, 2756, 1479, 5499, 3254,   94],\n",
       "        [2215, 5749,  398, 3651,   94, 4487, 3448, 5158,  277, 1324],\n",
       "        [1479, 2326, 4727, 4704, 4250, 1968, 6561, 4730,  909,   94],\n",
       "        [2697, 3855, 2650, 1006, 1872, 4516, 6037, 4704, 3164,   94],\n",
       "        [1479, 2993, 6287, 5106, 4077, 5106, 1479, 1832, 5660,   94],\n",
       "        [4704, 2515, 3962, 5660, 6588, 6416, 2647, 5158, 3564,   94],\n",
       "        [1479, 5100, 1137,  457, 5556, 1968,  510, 5158, 5118,   94],\n",
       "        [2597, 5795, 6287, 3972, 5109, 1968, 1432, 1546, 3353,   94],\n",
       "        [2597, 4291, 3383, 1463,  334,    9, 1479, 1570, 3493,   94],\n",
       "        [2348, 1271, 4246,  454, 1340, 5235, 2551, 4704, 2477,   94],\n",
       "        [6190, 5564,  218, 1968, 5082, 4487, 3155, 6266, 2765,   94],\n",
       "        [3448,  520, 6287, 4360, 6463, 5564, 3601, 1872, 6242,   94],\n",
       "        [5564, 1596, 6242, 1721, 4704, 4250, 1968, 1263, 6067,   94],\n",
       "        [5749, 4286, 6417, 4704, 3863, 5660, 6037,  457, 6253,   94],\n",
       "        [5564, 1836, 5106, 3691, 5364, 3200, 6149, 5499, 1394,   94],\n",
       "        [2597, 5121, 5158,  207, 1968, 2649, 2445, 5749,  909,   94],\n",
       "        [1479, 2249,  510, 6037, 1522, 5749, 5373, 3192, 5539,   94]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i had a lot of fun at the party .'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(en_vocab.decode(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'パーティー で は 大いに 楽し ん だ 。 </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(ja_vocab.decode(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ' '.join(en_vocab.decode(source))\n",
    "target = ' '.join(ja_vocab.decode(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2597, 6376, 5158, 6160, 3455, 2130, 5749, 4704,  385,   94],\n",
       "        [3448, 4487, 1757, 6053, 4730, 1968,  725, 5373, 6582,   94],\n",
       "        [1479, 2392, 4704, 5160, 5271,  457, 5235,  909, 2144,   94],\n",
       "        [4246, 5564, 6127, 1968, 1721, 6002, 6037, 4704, 3932, 1324],\n",
       "        [1515,  448, 2224,  125, 4755, 1137, 4248, 3431, 3365,   94],\n",
       "        [3448, 5660, 6037, 1522,  398, 2597, 4286, 3843, 1546,   94],\n",
       "        [1546,  684, 5749, 4704,  385, 5660, 4463, 6488,  149,   94],\n",
       "        [1845, 5150,  398, 1845, 5925, 6287, 5133, 4704, 4320,   94],\n",
       "        [2597, 2276, 1481, 5564, 5773, 3683, 3455, 2649, 6149,   94],\n",
       "        [2597, 3700, 6261, 6417, 2597, 2863, 6287, 4815, 5564,   94],\n",
       "        [1479, 5660, 4904, 5267, 1968, 3413, 4730, 4704, 5235,   94],\n",
       "        [2863, 5564, 2696, 6053, 6190, 4704, 1258, 5564, 2897, 1324],\n",
       "        [1845, 2249, 4666, 2224, 6582, 1515, 3455, 2997, 3875,   94],\n",
       "        [1515, 4487, 3326, 6417, 4248, 4704, 4670, 4487, 2515,   94],\n",
       "        [2597, 2392, 3894, 2515, 3823, 3455, 2734, 6037, 1522,   94],\n",
       "        [2597, 2392, 3407, 6417,  220, 2022, 4730, 4704, 5717,   94],\n",
       "        [3584, 6376, 2597, 5495,    9, 3448, 4127, 1968, 4938,   94],\n",
       "        [2597, 3700, 6261,   94, 2597, 1721, 2883, 2649, 2301,   94],\n",
       "        [1479, 2952, 3972, 1984, 6053, 4704,  937,   53, 3875,   94],\n",
       "        [3448, 4487,  383,  398, 1479, 1633, 1263, 1968, 1334,   94],\n",
       "        [2827, 5499, 4704, 3759, 1821, 3352, 6037, 4704, 4318, 1324],\n",
       "        [3691, 5082, 2597, 1432, 4704, 6315, 5564, 2650, 5659,   94],\n",
       "        [2827, 5082, 5564, 1450, 1968, 6127, 6053, 5158, 1563, 1324],\n",
       "        [3448, 4487, 1944, 3680, 3336, 1968, 3625, 6350, 1463,   94],\n",
       "        [4704, 4250, 6350, 1463, 4487,  398, 1479, 4487, 1808,   94],\n",
       "        [4704, 3564, 5461, 1137, 6037, 3430, 3455, 4704, 2758,   94],\n",
       "        [1479, 4792, 5804, 6037, 4704,  375, 3455,  457, 5518,   94],\n",
       "        [3117, 3383, 4207, 4704,  961, 1843, 3455, 5373,  912,   94],\n",
       "        [1479, 5660, 4704, 4286,  881, 2597,  970, 1968, 1481,   94],\n",
       "        [6149, 2141, 3455, 4543, 3972, 4293, 4295, 4286, 2358,   94],\n",
       "        [2597, 2863, 6287, 3120, 6350, 5564, 6037, 5373,  521,   94],\n",
       "        [ 125, 2650, 1051, 2120, 1968, 4652, 3455,  457, 4213,   94],\n",
       "        [2649, 3443, 4487, 1051,  961, 5749, 6081, 4704, 3717,   94],\n",
       "        [2827, 3681, 5564, 5082, 4522, 5564, 3843, 5158,  230, 1324],\n",
       "        [4704, 1474, 6037, 5373, 4034, 4246, 1757, 1968, 3348,   94],\n",
       "        [3448, 4487, 5200, 6053, 4730, 1968, 1370, 4704, 2341,   94],\n",
       "        [2597, 2249, 1263, 6417, 2879, 4704, 2232, 4487,  729,   94],\n",
       "        [2597, 2054, 6334, 6037, 4704, 2765, 5271, 5373, 1761,   94],\n",
       "        [2273, 3455, 4704, 1184, 3214, 5082, 5564, 1547,  710, 1324],\n",
       "        [3448, 4487, 1757, 6053, 5564, 1968, 1370, 5373, 2341,   94],\n",
       "        [3200, 4506, 4487, 1051, 5370, 6417, 2993, 6287, 1845, 1324],\n",
       "        [4704, 3326,  912, 1850, 6037, 5158, 6160, 3455, 1258,   94],\n",
       "        [4178, 1263, 1968, 3490, 1695, 4178, 1446, 1968,  601,   94],\n",
       "        [3448, 4727, 4730, 4881, 4900, 1968, 5082, 2649, 3356,   94],\n",
       "        [4178, 6534, 1968, 3061, 4704,  449, 3455, 4704, 4793,   94],\n",
       "        [1845, 4587, 1968,  218,  863, 5158, 5630, 1968,  373,   94],\n",
       "        [1479, 5879, 3448, 5749, 4704, 4235, 3455,  457, 5590,   94],\n",
       "        [5564, 2249, 2874, 5052, 5303, 1968, 4704, 3326, 5930,   94],\n",
       "        [1479, 4487, 5494,  773,  711, 1479, 4496, 1968,  510,   94],\n",
       "        [ 220, 4487, 3448,  398, 5564, 1446, 1968, 5165, 1968, 1324],\n",
       "        [2249, 5564, 3404, 4730, 5158, 2191, 1968, 2649, 2085, 1324],\n",
       "        [3448, 4487, 5208, 2073, 4295, 2597, 3843, 5564, 4286,   94],\n",
       "        [5373, 6582, 3349, 2137, 6287,  510, 6190,  398, 2159,   94],\n",
       "        [ 218, 5564, 5927, 4496, 1968, 1082, 6541, 4688, 3764, 1324],\n",
       "        [4178, 5870, 4704, 6222, 1577, 6417, 6118, 6287, 4178, 1324],\n",
       "        [1479, 3395, 4730, 1944, 5494,  711, 5208, 4710, 1452,   94],\n",
       "        [2597, 6397, 4704, 1132, 5499, 6127, 1968, 4690, 4497,  114],\n",
       "        [6067, 4246, 5158, 2698, 2617, 2047, 6037, 5373, 3021,   94],\n",
       "        [3192, 2804, 2647,    8,    9, 5564,    5, 4543, 2130,   94],\n",
       "        [5373,  428, 6067, 2650,   53, 4723, 3875, 6037,   10,   94],\n",
       "        [4487, 6067, 6390,  861, 5564, 3681, 1547, 1968,  647, 1324],\n",
       "        [2649, 3443, 5660, 1051, 4533, 6350, 4704, 5588, 3846,   94],\n",
       "        [3448, 5499, 2212, 3192, 4704,  387, 3431, 1968, 1012,   94],\n",
       "        [3448, 4487, 3192, 6053, 5564, 1968, 1263, 1968, 1012,   94],\n",
       "        [ 457, 2445, 2022, 1463, 1968, 4690, 4877, 5158, 1978,   94],\n",
       "        [3448, 5660, 2893, 1569, 2597,  725, 6037, 5158, 2894,   94],\n",
       "        [2597, 5660, 4225, 5749, 3150,  803, 4704, 5282, 2650,   94],\n",
       "        [1479, 2952, 5158, 4061, 2273, 4487, 1757, 1968, 3348,   94],\n",
       "        [1479, 5100, 5158, 5518, 6037, 3430, 3455, 2617, 3855,   94],\n",
       "        [2656, 2952, 6242, 3764, 3972, 5848, 6053, 5158, 4320,   94],\n",
       "        [5879, 5564, 2316, 6390, 2597, 2863, 2258, 5271, 6458, 1324],\n",
       "        [2597, 1533,  909, 4157, 2774, 5173, 3513, 1824, 4312,   94],\n",
       "        [1479, 5692,  457, 2515, 3564, 6053, 5158, 3326, 1515,   94],\n",
       "        [1479, 2952,  253, 6417,  334, 1479, 2952, 2617,  261,   94],\n",
       "        [3448, 4487, 2893,  398, 4178, 1633, 6476,  457, 1403,   94],\n",
       "        [2597,  218,  744, 4130, 5106, 2617, 2047, 5106, 1479,   94],\n",
       "        [2649, 6149,   46, 6141, 6037, 5158, 1051,   80,  287,   94],\n",
       "        [5373, 4487, 5208, 4710, 1452, 6037, 4704, 6541, 1452,   94],\n",
       "        [1479, 5268,  448,  883, 3265, 1657, 4178, 2650, 3365,   94],\n",
       "        [4178, 6024, 1944, 3192, 1525, 3365, 3455, 4704, 2440,   94],\n",
       "        [4704, 3962, 4510, 6053, 4704, 3247, 6024,  457, 1884,   94],\n",
       "        [4178, 2249, 6190, 4094, 2647, 5564, 1968, 4704, 4286,   94],\n",
       "        [1479, 5660, 5749, 5158, 1018, 2273, 5235, 1968, 1721,   94],\n",
       "        [1479, 5499, 6242,  402, 6417, 2756, 1479, 5499, 3254,   94],\n",
       "        [2215, 5749,  398, 3651,   94, 4487, 3448, 5158,  277, 1324],\n",
       "        [1479, 2326, 4727, 4704, 4250, 1968, 6561, 4730,  909,   94],\n",
       "        [2697, 3855, 2650, 1006, 1872, 4516, 6037, 4704, 3164,   94],\n",
       "        [1479, 2993, 6287, 5106, 4077, 5106, 1479, 1832, 5660,   94],\n",
       "        [4704, 2515, 3962, 5660, 6588, 6416, 2647, 5158, 3564,   94],\n",
       "        [1479, 5100, 1137,  457, 5556, 1968,  510, 5158, 5118,   94],\n",
       "        [2597, 5795, 6287, 3972, 5109, 1968, 1432, 1546, 3353,   94],\n",
       "        [2597, 4291, 3383, 1463,  334,    9, 1479, 1570, 3493,   94],\n",
       "        [2348, 1271, 4246,  454, 1340, 5235, 2551, 4704, 2477,   94],\n",
       "        [6190, 5564,  218, 1968, 5082, 4487, 3155, 6266, 2765,   94],\n",
       "        [3448,  520, 6287, 4360, 6463, 5564, 3601, 1872, 6242,   94],\n",
       "        [5564, 1596, 6242, 1721, 4704, 4250, 1968, 1263, 6067,   94],\n",
       "        [5749, 4286, 6417, 4704, 3863, 5660, 6037,  457, 6253,   94],\n",
       "        [5564, 1836, 5106, 3691, 5364, 3200, 6149, 5499, 1394,   94],\n",
       "        [2597, 5121, 5158,  207, 1968, 2649, 2445, 5749,  909,   94],\n",
       "        [1479, 2249,  510, 6037, 1522, 5749, 5373, 3192, 5539,   94]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "# データの定義\n",
    "depth_x = len(en_vocab.i2w)\n",
    "depth_t = len(ja_vocab.i2w)\n",
    "input_dim = depth_x\n",
    "hidden_dim = 128\n",
    "output_dim = depth_t\n",
    "\n",
    "# アルゴリズムの定義\n",
    "epochs = 30\n",
    "model = EncoderDecoder(input_dim, hidden_dim, output_dim, device=device).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=0)\n",
    "optimizer = optimizers.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), amsgrad=True)\n",
    "\n",
    "def compute_loss(label, pred):\n",
    "    return criterion(pred, label)\n",
    "\n",
    "def train_step(x, t, teacher_forcing_rate=0.5):\n",
    "    use_teacher_forcing = (random.random() < teacher_forcing_rate)\n",
    "    model.train()\n",
    "    preds = model(x, t, use_teacher_forcing=use_teacher_forcing)\n",
    "    loss = compute_loss(t.reshape(-1), preds.view(-1, preds.size(-1)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds\n",
    "\n",
    "def val_step(x, t):\n",
    "    model.eval()\n",
    "    preds = model(x, t, use_teacher_forcing=False)\n",
    "    loss = compute_loss(t.reshape(-1), preds.view(-1, preds.size(-1)))\n",
    "    return loss, preds\n",
    "\n",
    "def test_step(x):\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_rate=0.5\n",
    "use_teacher_forcing = (random.random() < teacher_forcing_rate)\n",
    "a1 = model(x, t, use_teacher_forcing=use_teacher_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f905ffc47dae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a247e61c764d>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x, t, teacher_forcing_rate)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 確率的勾配法でモデルを学習\n",
    "for epoch in range(epochs):\n",
    "    print(\"-\" * 20)\n",
    "    print(\"epoch: {}\".format(epoch+1))\n",
    "\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    for (x, t) in train_dataloader:\n",
    "        loss, _ = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    for (x, t) in val_dataloader:\n",
    "        loss, _ = val_step(x, t)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print('loss: {:.3f}, val_loss: {:.3}'.format(\n",
    "        train_loss,\n",
    "        val_loss\n",
    "    ))\n",
    "\n",
    "    for idx, (x, t) in enumerate(test_dataloader):\n",
    "        preds = test_step(x)\n",
    "\n",
    "        source = x.T.reshape(-1).tolist()\n",
    "        target = t.T.reshape(-1).tolist()\n",
    "        out = preds.max(dim=-1)[1].T.reshape(-1).tolist()\n",
    "\n",
    "        source = ' '.join(en_vocab.decode(source))\n",
    "        target = ' '.join(ja_vocab.decode(target))\n",
    "        out = ' '.join(ja_vocab.decode(out))\n",
    "\n",
    "        print('>', source)\n",
    "        print('=', target)\n",
    "        print('<', out)\n",
    "        print()\n",
    "\n",
    "        if idx >= 9:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
