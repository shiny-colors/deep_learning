{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.nn.functional as F \n",
    "import torchtext\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from torchtext.vocab import Vectors\n",
    "from utils.dataloader import get_IMDb_DataLoaders_and_TEXT\n",
    "\n",
    "np.random.seed(9837)\n",
    "torch.manual_seed(9837)\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理の関数\n",
    "def preprocessing_text(text):\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    return text\n",
    "\n",
    "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "\n",
    "# 前処理と分かち書きをまとめた関数を定義\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データのtsvファイルを作成\n",
    "path = \"D:/Statistics/data/deep_leraning/nlp/transformer/\"\n",
    "\n",
    "f = open(path + \"IMDb_train.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "positive_path = path + \"aclImdb/train/pos/\"\n",
    "for fname in glob.glob(os.path.join(positive_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"1\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "negative_path = path + \"aclImdb/train/neg/\"\n",
    "for fname in glob.glob(os.path.join(negative_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"0\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "f.close()\n",
    "\n",
    "# テストデータのtsvファイルを作成\n",
    "f = open(path + \"IMDb_test.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "positive_path = path + \"aclImdb/test/pos/\"\n",
    "for fname in glob.glob(os.path.join(positive_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"1\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "negative_path = path + \"aclImdb/test/neg/\"\n",
    "for fname in glob.glob(os.path.join(negative_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"0\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textとラベルを定義\n",
    "# 文章とラベルの両方を用意\n",
    "max_length = 256\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, \n",
    "                            init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# フォルダ「data」からtsvファイルを読み込み\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(path=path, train=\"IMDb_train.tsv\", test=\"IMDb_test.tsv\", format=\"tsv\",\n",
    "                                                             fields=[(\"Text\", TEXT), (\"Label\", LABEL)])\n",
    "\n",
    "# torchtext.data.Datasetのsplit関数で訓練データと検証データに分割\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "\n",
    "# ボキャブラリーを作成\n",
    "# torchtextで単語ベクトルとして英語学習済みモデルを読み込み\n",
    "load_path = path + \"wiki-news-300d-1M.vec\" \n",
    "english_fasttext_vectors = Vectors(name=load_path)\n",
    "\n",
    "# ベクトル化したバージョンのボキャブラリーを作成\n",
    "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "# DataLoaderを作成\n",
    "batch_size = 64\n",
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "val_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}   # 辞書オブジェクトにまとめる\n",
    "\n",
    "# ミニバッチの用意\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerのblockを定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding層モジュールを定義\n",
    "class Embedder(nn.Module):\n",
    "    # idで示される単語をベクトルに変換\n",
    "    \n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        # 学習済み単語ベクトルを読み込み(freeze=Trueで学習しない)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings=text_embedding_vectors, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_vec = self.embeddings(x)\n",
    "        return x_vec        \n",
    "    \n",
    "# 動作を確認\n",
    "word_id = batch.Text[0]\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "x = net1(word_id)   # 単語をベクトルに\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", word_id.shape)\n",
    "print(\"出力のテンソルサイズ：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoder層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoder層モジュールを定義\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \n",
    "    # 入力された単語の位置を示すベクトル情報を付加する\n",
    "    def __init__(self, d_model=300, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model   # 単語ベクトルの次元数\n",
    "        \n",
    "        # 単語の順序と埋め込みベクトルが一意に定まる値を定義\n",
    "        pe = torch.zeros((max_seq_len, d_model))\n",
    "        \n",
    "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        pe = pe.to(device)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = scipy.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i+1] = scipy.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False   # 勾配は計算しない\n",
    "       \n",
    "    # 入力する単語ベクトルとPositional Embeddingの和を取る\n",
    "    def forward(self, x):\n",
    "        ret = np.sqrt(self.d_model)*x + self.pe\n",
    "        return ret\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "\n",
    "# 入出力\n",
    "word_id = batch.Text[0]\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "print(\"入力のテンソルサイズ：\", x1.shape)\n",
    "print(\"出力のテンソルサイズ：\", x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention層モジュールを定義\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 全結合層で特徴量を変換\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 出力時の全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # attentionの大きさを調整\n",
    "        self.d_k = d_model\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "        \n",
    "        # attentionの値を計算\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # mask計算\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        # softmaxで規格化\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        # AttentionとValueの積\n",
    "        output = torch.matmul(normalized_weights, v)\n",
    "        \n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, normalized_weights, k, q, v\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = Attention(d_model=300)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "output, normalized_weights, k, q, v = net3(x2, x2, x2, input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer Block層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerブロックを定義\n",
    "# Feedforward層を定義\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "# Transformer層を定義\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LayerNormalization層\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "        \n",
    "        # 全結合層\n",
    "        self.ff = FeedForward(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # normalizationとattention\n",
    "        x_normalized1 = self.norm1(x)\n",
    "        output, normalized_weights, k, q, v = self.attn(x_normalized1, x_normalized1, x_normalized1, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "        \n",
    "        # 正規化と全結合層\n",
    "        x_normalized2 = self.norm2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normalized2))\n",
    "        return output, normalized_weights\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "x3, normalized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x2.shape)\n",
    "print(\"出力のテンソルサイズ：\", x3.shape)\n",
    "print(\"Attentionのサイズ：\", normalized_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Head層を定義\n",
    "class ClassificationHead(nn.Module):\n",
    "    # Transformer Blockの出力を使用し、最後にクラス分類させる\n",
    "    def __init__(self, d_model=300, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)   # output dimはポジティブ、ネガティブの2つ\n",
    "        \n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Z = x[:, 0, :]   # 各ミニバッチの各文の先頭の単語の特徴量(300次元)を取り出す\n",
    "        out = self.linear(Z)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "net4 = ClassificationHead(d_model=300, output_dim=2)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "x3, normalized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "x4 = net4(x3)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x3.shape)\n",
    "print(\"出力のテンソルサイズ：\", x4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for batch in (dataloaders_dict[\"train\"]):\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    test.append(batch.Text[0].numpy().astype(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(np.vstack((test)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformerモデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformerを実装\n",
    "class TransformerClassification(nn.Module):\n",
    "    # Transformerでクラス分類させる\n",
    "    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # モデル構築\n",
    "        self.net1 = Embedder(text_embedding_vectors)\n",
    "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
    "        self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        self.net4 = ClassificationHead(d_model=d_model, output_dim=output_dim)\n",
    "        \n",
    "    def forward(self, word_id, mask):\n",
    "        x1 = self.net1(word_id)   # 単語をベクトルに\n",
    "        x2 = self.net2(x1)   # position情報の和を取る\n",
    "        x3_1, normalized_weights_1 = self.net3_1(x2, input_mask)  # 1つ目のSelf Attentionで特徴量を変換\n",
    "        x3_2, normalized_weights_2 = self.net3_2(x3_1, input_mask)   # 2つ目のSelf Attentionで特徴量を変換\n",
    "        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        return x4, normalized_weights_1, normalized_weights_2\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net = TransformerClassification(text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "out, normalized_weights_1, normalized_weights_2 = net(word_id, input_mask)\n",
    "\n",
    "print(\"出力のテンソルサイズ：\", out.shape)\n",
    "print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの学習と推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの初期化関数を定義\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\")!=-1:\n",
    "        # Linear層の初期化\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# ネットワークモデルを定義\n",
    "net = TransformerClassification(text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "net.net3_1.apply(weights_init)\n",
    "net.net3_2.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 負の対数尤度関数と最適化手法を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負の対数尤度関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法の設定\n",
    "learning_rate = 2e-5\n",
    "optimizer = optimizers.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アルゴリズムの設定\n",
    "num_epochs = 15\n",
    "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "\n",
    "# GPUが使えるかを確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用デバイス：\", device)\n",
    "print('-----start-------')\n",
    "# ネットワークをGPUへ\n",
    "net.to(device)\n",
    "\n",
    "# ネットワークがある程度固定であれば、高速化させる\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# epochのループ\n",
    "for epoch in range(num_epochs):\n",
    "    # epochごとの訓練と検証のループ\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase=='train':\n",
    "            net.train()  # モデルを訓練モードに\n",
    "        else:\n",
    "            net.eval()   # モデルを検証モードに\n",
    "\n",
    "        epoch_loss = 0.0  # epochの損失和\n",
    "        epoch_corrects = 0  # epochの正解数\n",
    "\n",
    "        # データローダーからミニバッチを取り出すループ\n",
    "        for batch in (dataloaders_dict[phase]):\n",
    "            # batchはTextとLableの辞書オブジェクト\n",
    "\n",
    "            # GPUが使えるならGPUにデータを送る\n",
    "            inputs = batch.Text[0].to(device)  # 文章\n",
    "            labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "            # optimizerを初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 順伝搬（forward）計算\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                # mask作成\n",
    "                input_mask = (inputs!=input_pad)\n",
    "\n",
    "                # Transformerに入力\n",
    "                outputs, _, _ = net(inputs, input_mask)\n",
    "                loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                # 訓練時はバックプロパゲーション\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # 結果の計算\n",
    "                epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
    "                # 正解数の合計を更新\n",
    "                epoch_corrects += torch.sum(preds==labels.data)\n",
    "\n",
    "        # epochごとのlossと正解率\n",
    "        epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "        epoch_acc = epoch_corrects.double(\n",
    "        ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "        print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                       phase, epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
