{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.nn.functional as F \n",
    "import torchtext\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from torchtext.vocab import Vectors\n",
    "from utils.dataloader import get_IMDb_DataLoaders_and_TEXT\n",
    "\n",
    "np.random.seed(9837)\n",
    "torch.manual_seed(9837)\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理の関数\n",
    "def preprocessing_text(text):\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    return text\n",
    "\n",
    "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "\n",
    "# 前処理と分かち書きをまとめた関数を定義\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データのtsvファイルを作成\n",
    "path = \"D:/Statistics/data/deep_leraning/nlp/\"\n",
    "\n",
    "f = open(path + \"IMDb_train.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "positive_path = path + \"aclImdb/train/pos/\"\n",
    "for fname in glob.glob(os.path.join(positive_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"1\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "negative_path = path + \"aclImdb/train/neg/\"\n",
    "for fname in glob.glob(os.path.join(negative_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"0\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "f.close()\n",
    "\n",
    "# テストデータのtsvファイルを作成\n",
    "f = open(path + \"IMDb_test.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "positive_path = path + \"aclImdb/test/pos/\"\n",
    "for fname in glob.glob(os.path.join(positive_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"1\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "negative_path = path + \"aclImdb/test/neg/\"\n",
    "for fname in glob.glob(os.path.join(negative_path, \"*.txt\")):\n",
    "    with io.open(fname, \"r\", encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消去\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "        text = text+\"\\t\"+\"0\"+\"\\t\"+\"\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textとラベルを定義\n",
    "# 文章とラベルの両方を用意\n",
    "max_length=256\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, \n",
    "                            init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# フォルダ「data」からtsvファイルを読み込み\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(path=path, train=\"IMDb_train.tsv\", test=\"IMDb_test.tsv\", format=\"tsv\",\n",
    "                                                             fields=[(\"Text\", TEXT), (\"Label\", LABEL)])\n",
    "\n",
    "# torchtext.data.Datasetのsplit関数で訓練データと検証データに分割\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "\n",
    "# ボキャブラリーを作成\n",
    "# torchtextで単語ベクトルとして英語学習済みモデルを読み込み\n",
    "load_path = path + \"wiki-news-300d-1M.vec\" \n",
    "english_fasttext_vectors = Vectors(name=load_path)\n",
    "\n",
    "# ベクトル化したバージョンのボキャブラリーを作成\n",
    "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "# DataLoaderを作成\n",
    "batch_size = 64\n",
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "val_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}   # 辞書オブジェクトにまとめる\n",
    "\n",
    "# ミニバッチの用意\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerのblockを定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([64, 256])\n",
      "出力のテンソルサイズ： torch.Size([64, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "# Embedding層モジュールを定義\n",
    "class Embedder(nn.Module):\n",
    "    # idで示される単語をベクトルに変換\n",
    "    \n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        # 学習済み単語ベクトルを読み込み(freeze=Trueで学習しない)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings=text_embedding_vectors, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_vec = self.embeddings(x)\n",
    "        return x_vec        \n",
    "    \n",
    "# 動作を確認\n",
    "word_id = batch.Text[0]\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "x = net1(word_id)   # 単語をベクトルに\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", word_id.shape)\n",
    "print(\"出力のテンソルサイズ：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoder層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([64, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([64, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoder層モジュールを定義\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \n",
    "    # 入力された単語の位置を示すベクトル情報を付加する\n",
    "    def __init__(self, d_model=300, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model   # 単語ベクトルの次元数\n",
    "        \n",
    "        # 単語の順序と埋め込みベクトルが一意に定まる値を定義\n",
    "        pe = torch.zeros((max_seq_len, d_model))\n",
    "        \n",
    "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        pe = pe.to(device)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = scipy.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i+1] = scipy.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False   # 勾配は計算しない\n",
    "       \n",
    "    # 入力する単語ベクトルとPositional Embeddingの和を取る\n",
    "    def forward(self, x):\n",
    "        ret = np.sqrt(self.d_model)*x + self.pe\n",
    "        return ret\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "\n",
    "# 入出力\n",
    "word_id = batch.Text[0]\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "print(\"入力のテンソルサイズ：\", x1.shape)\n",
    "print(\"出力のテンソルサイズ：\", x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention層モジュールを定義\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 全結合層で特徴量を変換\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 出力時の全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # attentionの大きさを調整\n",
    "        self.d_k = d_model\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "        \n",
    "        # attentionの値を計算\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # mask計算\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        # softmaxで規格化\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        # AttentionとValueの積\n",
    "        output = torch.matmul(normalized_weights, v)\n",
    "        \n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, normalized_weights, k, q, v\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = Attention(d_model=300)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "output, normalized_weights, k, q, v = net3(x2, x2, x2, input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer Block層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([64, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([64, 256, 300])\n",
      "Attentionのサイズ： torch.Size([64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Transformerブロックを定義\n",
    "# Feedforward層を定義\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "# Transformer層を定義\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LayerNormalization層\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "        \n",
    "        # 全結合層\n",
    "        self.ff = FeedForward(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # normalizationとattention\n",
    "        x_normalized1 = self.norm1(x)\n",
    "        output, normalized_weights, k, q, v = self.attn(x_normalized1, x_normalized1, x_normalized1, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "        \n",
    "        # 正規化と全結合層\n",
    "        x_normalized2 = self.norm2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normalized2))\n",
    "        return output, normalized_weights\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "x3, normalized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x2.shape)\n",
    "print(\"出力のテンソルサイズ：\", x3.shape)\n",
    "print(\"Attentionのサイズ：\", normalized_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([64, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "# Classification Head層を定義\n",
    "class ClassificationHead(nn.Module):\n",
    "    # Transformer Blockの出力を使用し、最後にクラス分類させる\n",
    "    def __init__(self, d_model=300, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)   # output dimはポジティブ、ネガティブの2つ\n",
    "        \n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Z = x[:, 0, :]   # 各ミニバッチの各文の先頭の単語の特徴量(300次元)を取り出す\n",
    "        out = self.linear(Z)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "net4 = ClassificationHead(d_model=300, output_dim=2)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "x1 = net1(word_id)\n",
    "x2 = net2(x1)\n",
    "x3, normalized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "x4 = net4(x3)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x3.shape)\n",
    "print(\"出力のテンソルサイズ：\", x4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformerモデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力のテンソルサイズ： torch.Size([64, 2])\n",
      "出力テンソルのsigmoid： tensor([[0.2225, 0.7775],\n",
      "        [0.2317, 0.7683],\n",
      "        [0.1983, 0.8017],\n",
      "        [0.2279, 0.7721],\n",
      "        [0.2064, 0.7936],\n",
      "        [0.2529, 0.7471],\n",
      "        [0.2008, 0.7992],\n",
      "        [0.2313, 0.7687],\n",
      "        [0.2142, 0.7858],\n",
      "        [0.2288, 0.7712],\n",
      "        [0.2268, 0.7732],\n",
      "        [0.2173, 0.7827],\n",
      "        [0.1908, 0.8092],\n",
      "        [0.2280, 0.7720],\n",
      "        [0.2069, 0.7931],\n",
      "        [0.2108, 0.7892],\n",
      "        [0.2073, 0.7927],\n",
      "        [0.2060, 0.7940],\n",
      "        [0.2272, 0.7728],\n",
      "        [0.2117, 0.7883],\n",
      "        [0.2248, 0.7752],\n",
      "        [0.2349, 0.7651],\n",
      "        [0.2618, 0.7382],\n",
      "        [0.2182, 0.7818],\n",
      "        [0.2629, 0.7371],\n",
      "        [0.2112, 0.7888],\n",
      "        [0.2155, 0.7845],\n",
      "        [0.1992, 0.8008],\n",
      "        [0.2212, 0.7788],\n",
      "        [0.2354, 0.7646],\n",
      "        [0.2339, 0.7661],\n",
      "        [0.2291, 0.7709],\n",
      "        [0.2097, 0.7903],\n",
      "        [0.1909, 0.8091],\n",
      "        [0.2479, 0.7521],\n",
      "        [0.2107, 0.7893],\n",
      "        [0.2061, 0.7939],\n",
      "        [0.2170, 0.7830],\n",
      "        [0.2196, 0.7804],\n",
      "        [0.2359, 0.7641],\n",
      "        [0.2247, 0.7753],\n",
      "        [0.2228, 0.7772],\n",
      "        [0.2313, 0.7687],\n",
      "        [0.2197, 0.7803],\n",
      "        [0.2223, 0.7777],\n",
      "        [0.2130, 0.7870],\n",
      "        [0.2188, 0.7812],\n",
      "        [0.2107, 0.7893],\n",
      "        [0.2544, 0.7456],\n",
      "        [0.2063, 0.7937],\n",
      "        [0.2295, 0.7705],\n",
      "        [0.2606, 0.7394],\n",
      "        [0.2129, 0.7871],\n",
      "        [0.1959, 0.8041],\n",
      "        [0.2496, 0.7504],\n",
      "        [0.2335, 0.7665],\n",
      "        [0.2540, 0.7460],\n",
      "        [0.2250, 0.7750],\n",
      "        [0.2161, 0.7839],\n",
      "        [0.2242, 0.7758],\n",
      "        [0.2042, 0.7958],\n",
      "        [0.1970, 0.8030],\n",
      "        [0.2397, 0.7603],\n",
      "        [0.1989, 0.8011]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Transformerを実装\n",
    "class TransformerClassification(nn.Module):\n",
    "    # Transformerでクラス分類させる\n",
    "    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # モデル構築\n",
    "        self.net1 = Embedder(text_embedding_vectors)\n",
    "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
    "        self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        self.net4 = ClassificationHead(d_model=d_model, output_dim=output_dim)\n",
    "        \n",
    "    def forward(self, word_id, mask):\n",
    "        x1 = self.net1(word_id)   # 単語をベクトルに\n",
    "        x2 = self.net2(x1)   # position情報の和を取る\n",
    "        x3_1, normalized_weights_1 = self.net3_1(x2, input_mask)  # 1つ目のSelf Attentionで特徴量を変換\n",
    "        x3_2, normalized_weights_2 = self.net3_2(x3_1, input_mask)   # 2つ目のSelf Attentionで特徴量を変換\n",
    "        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        return x4, normalized_weights_1, normalized_weights_2\n",
    "    \n",
    "# 動作確認\n",
    "# モデル構築\n",
    "net = TransformerClassification(text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
    "\n",
    "# 入力を定義\n",
    "word_id = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (word_id!=input_pad)\n",
    "\n",
    "# モデル検証\n",
    "out, normalized_weights_1, normalized_weights_2 = net(word_id, input_mask)\n",
    "\n",
    "print(\"出力のテンソルサイズ：\", out.shape)\n",
    "print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの学習と推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): Attention(\n",
       "    (q_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (v_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (k_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ネットワークの初期化関数を定義\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\")!=-1:\n",
    "        # Linear層の初期化\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# ネットワークモデルを定義\n",
    "net = TransformerClassification(text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "net.net3_1.apply(weights_init)\n",
    "net.net3_2.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 負の対数尤度関数と最適化手法を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負の対数尤度関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法の設定\n",
    "learning_rate = 2e-5\n",
    "optimizer = optimizers.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cpu\n",
      "-----start-------\n",
      "Epoch 1/30 | train |  Loss: 0.6300 Acc: 0.6379\n",
      "Epoch 1/30 |  val  |  Loss: 0.4352 Acc: 0.8007\n",
      "Epoch 2/30 | train |  Loss: 0.4413 Acc: 0.7983\n",
      "Epoch 2/30 |  val  |  Loss: 0.3946 Acc: 0.8230\n",
      "Epoch 3/30 | train |  Loss: 0.4055 Acc: 0.8174\n",
      "Epoch 3/30 |  val  |  Loss: 0.3757 Acc: 0.8339\n",
      "Epoch 4/30 | train |  Loss: 0.3833 Acc: 0.8292\n",
      "Epoch 4/30 |  val  |  Loss: 0.3658 Acc: 0.8392\n",
      "Epoch 5/30 | train |  Loss: 0.3718 Acc: 0.8357\n",
      "Epoch 5/30 |  val  |  Loss: 0.3628 Acc: 0.8421\n",
      "Epoch 6/30 | train |  Loss: 0.3611 Acc: 0.8419\n",
      "Epoch 6/30 |  val  |  Loss: 0.3534 Acc: 0.8465\n",
      "Epoch 7/30 | train |  Loss: 0.3526 Acc: 0.8502\n",
      "Epoch 7/30 |  val  |  Loss: 0.3500 Acc: 0.8476\n",
      "Epoch 8/30 | train |  Loss: 0.3437 Acc: 0.8524\n",
      "Epoch 8/30 |  val  |  Loss: 0.3444 Acc: 0.8487\n",
      "Epoch 9/30 | train |  Loss: 0.3378 Acc: 0.8526\n",
      "Epoch 9/30 |  val  |  Loss: 0.3418 Acc: 0.8504\n",
      "Epoch 10/30 | train |  Loss: 0.3322 Acc: 0.8563\n",
      "Epoch 10/30 |  val  |  Loss: 0.3400 Acc: 0.8515\n",
      "Epoch 11/30 | train |  Loss: 0.3222 Acc: 0.8616\n",
      "Epoch 11/30 |  val  |  Loss: 0.3354 Acc: 0.8537\n",
      "Epoch 12/30 | train |  Loss: 0.3180 Acc: 0.8657\n",
      "Epoch 12/30 |  val  |  Loss: 0.3494 Acc: 0.8503\n",
      "Epoch 13/30 | train |  Loss: 0.3136 Acc: 0.8654\n",
      "Epoch 13/30 |  val  |  Loss: 0.3440 Acc: 0.8502\n",
      "Epoch 14/30 | train |  Loss: 0.3113 Acc: 0.8666\n",
      "Epoch 14/30 |  val  |  Loss: 0.3397 Acc: 0.8532\n",
      "Epoch 15/30 | train |  Loss: 0.3024 Acc: 0.8712\n",
      "Epoch 15/30 |  val  |  Loss: 0.3408 Acc: 0.8532\n",
      "Epoch 16/30 | train |  Loss: 0.3037 Acc: 0.8730\n",
      "Epoch 16/30 |  val  |  Loss: 0.3486 Acc: 0.8495\n",
      "Epoch 17/30 | train |  Loss: 0.2900 Acc: 0.8781\n",
      "Epoch 17/30 |  val  |  Loss: 0.3553 Acc: 0.8489\n",
      "Epoch 18/30 | train |  Loss: 0.2905 Acc: 0.8761\n",
      "Epoch 18/30 |  val  |  Loss: 0.3444 Acc: 0.8540\n",
      "Epoch 19/30 | train |  Loss: 0.2878 Acc: 0.8767\n",
      "Epoch 19/30 |  val  |  Loss: 0.3493 Acc: 0.8515\n",
      "Epoch 20/30 | train |  Loss: 0.2828 Acc: 0.8789\n",
      "Epoch 20/30 |  val  |  Loss: 0.3548 Acc: 0.8518\n",
      "Epoch 21/30 | train |  Loss: 0.2781 Acc: 0.8801\n",
      "Epoch 21/30 |  val  |  Loss: 0.3460 Acc: 0.8551\n",
      "Epoch 22/30 | train |  Loss: 0.2743 Acc: 0.8847\n",
      "Epoch 22/30 |  val  |  Loss: 0.3520 Acc: 0.8519\n",
      "Epoch 23/30 | train |  Loss: 0.2707 Acc: 0.8848\n",
      "Epoch 23/30 |  val  |  Loss: 0.3544 Acc: 0.8544\n",
      "Epoch 24/30 | train |  Loss: 0.2677 Acc: 0.8850\n",
      "Epoch 24/30 |  val  |  Loss: 0.3533 Acc: 0.8523\n",
      "Epoch 25/30 | train |  Loss: 0.2692 Acc: 0.8838\n",
      "Epoch 25/30 |  val  |  Loss: 0.3582 Acc: 0.8533\n",
      "Epoch 26/30 | train |  Loss: 0.2585 Acc: 0.8924\n",
      "Epoch 26/30 |  val  |  Loss: 0.3654 Acc: 0.8521\n",
      "Epoch 27/30 | train |  Loss: 0.2538 Acc: 0.8940\n",
      "Epoch 27/30 |  val  |  Loss: 0.3657 Acc: 0.8488\n",
      "Epoch 28/30 | train |  Loss: 0.2485 Acc: 0.8950\n",
      "Epoch 28/30 |  val  |  Loss: 0.3711 Acc: 0.8507\n",
      "Epoch 29/30 | train |  Loss: 0.2467 Acc: 0.8954\n",
      "Epoch 29/30 |  val  |  Loss: 0.3860 Acc: 0.8440\n",
      "Epoch 30/30 | train |  Loss: 0.2422 Acc: 0.9004\n",
      "Epoch 30/30 |  val  |  Loss: 0.3814 Acc: 0.8497\n"
     ]
    }
   ],
   "source": [
    "# アルゴリズムの設定\n",
    "num_epochs = 15\n",
    "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "\n",
    "# GPUが使えるかを確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用デバイス：\", device)\n",
    "print('-----start-------')\n",
    "# ネットワークをGPUへ\n",
    "net.to(device)\n",
    "\n",
    "# ネットワークがある程度固定であれば、高速化させる\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# epochのループ\n",
    "for epoch in range(num_epochs):\n",
    "    # epochごとの訓練と検証のループ\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase=='train':\n",
    "            net.train()  # モデルを訓練モードに\n",
    "        else:\n",
    "            net.eval()   # モデルを検証モードに\n",
    "\n",
    "        epoch_loss = 0.0  # epochの損失和\n",
    "        epoch_corrects = 0  # epochの正解数\n",
    "\n",
    "        # データローダーからミニバッチを取り出すループ\n",
    "        for batch in (dataloaders_dict[phase]):\n",
    "            # batchはTextとLableの辞書オブジェクト\n",
    "\n",
    "            # GPUが使えるならGPUにデータを送る\n",
    "            inputs = batch.Text[0].to(device)  # 文章\n",
    "            labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "            # optimizerを初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 順伝搬（forward）計算\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                # mask作成\n",
    "                input_mask = (inputs!=input_pad)\n",
    "\n",
    "                # Transformerに入力\n",
    "                outputs, _, _ = net(inputs, input_mask)\n",
    "                loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                # 訓練時はバックプロパゲーション\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # 結果の計算\n",
    "                epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
    "                # 正解数の合計を更新\n",
    "                epoch_corrects += torch.sum(preds==labels.data)\n",
    "\n",
    "        # epochごとのlossと正解率\n",
    "        epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "        epoch_acc = epoch_corrects.double(\n",
    "        ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "        print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                       phase, epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
