{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.nn.functional as F \n",
    "import torchtext\n",
    "import glob\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tarfile\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "np.random.seed(9837)\n",
    "torch.manual_seed(9837)\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTのデータファイルを準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのダウンロード\n",
    "# フォルダを作成\n",
    "path = \"D:/Statistics/data/deep_leraning/nlp/BERT/\"\n",
    "vocab_path = path + \"/vocab/\"\n",
    "weights_path = path + \"/weights/\"\n",
    "if not os.path.exists(vocab_path):\n",
    "    os.mkdir(vocab_path)\n",
    "if not os.path.exists(weights_path):\n",
    "    os.mkdir(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語、語彙をダウンロード\n",
    "save_path = path + \"/vocab/bert-base-uncased-vocab.txt\"\n",
    "if os.path.isfile(save_path)==False:\n",
    "    url = \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\"\n",
    "    urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTの学習済みモデルをダウンロード\n",
    "# ダウンロード\n",
    "save_path = path + \"/weights/bert-base-uncased.tar.gz\"\n",
    "if os.path.isfile(save_path)==False:\n",
    "    url = \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\"\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "# 解凍\n",
    "archive_file = path + \"/weights/bert-base-uncased.tar.gz\"  # Uncasedは小文字化モードという意味です\n",
    "tar = tarfile.open(archive_file, 'r:gz')\n",
    "tar.extractall(weights_path)  # 解凍\n",
    "tar.close()  # ファイルをクローズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbデータセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDbデータセットをダウンロード\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "save_path = path + \"/aclImdb_v1.tar.gz\"\n",
    "if os.path.isfile(save_path)==False:\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "# tarファイルを読み込み\n",
    "tar = tarfile.open(path + \"/aclImdb_v1.tar.gz\")\n",
    "tar.extractall(path)  # 解凍\n",
    "tar.close()  # ファイルをクローズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDbの個別ファイルをtsvにまとめる\n",
    "target_path = path + \"/aclImdb/\"\n",
    "\n",
    "if os.path.exists(target_path):\n",
    "    \n",
    "    # 訓練データの作成\n",
    "    f = open(path + \"/IMDb_train.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    pos_path = path + '/aclImdb/train/pos/'\n",
    "    for fname in glob.glob(os.path.join(pos_path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "            \n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "            \n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    neg_path = path + \"aclImdb/train/neg/\"\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "            \n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "            \n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    # テストデータの作成\n",
    "    f = open(path + \"/IMDb_test.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    pos_path = path + \"/aclImdb/test/pos/\"\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "            \n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "        \n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    neg_path = path + \"/aclImdb/test/neg/\"\n",
    "\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "            \n",
    "            \n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "            \n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTのネットワークの設定ファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from attrdict import AttrDict\n",
    "\n",
    "# ファイルを開き、jsonファイルとして読み込む\n",
    "config_file = path + \"/weights/bert_config.json\"\n",
    "json_file = open(config_file, \"r\")\n",
    "config = json.load(json_file)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 辞書変数をオブジェクト変数に\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT用にLayer Normalization層を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization層を定義\n",
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))   # weightパラメータ\n",
    "        self.beta = nn.Parameter(torch.zeors(hidden_size))   # biasパラメータ\n",
    "        self.variance_epsilon = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingsモジュールを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層の実装\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        \n",
    "        # Token_Embedding: 単語ベクトルを定義\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "\n",
    "        # Transformer Positional Embedding: 位置情報ベクトルを定義\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        \n",
    "        # Sentence Embedding: 文章ベクトルの定義\n",
    "        self.tokey_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # Layer Normalization層\n",
    "        self.Layernorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, input_ids, token_type, type_ids=None):\n",
    "        # 1. Token Embeddings\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "\n",
    "        # 2. Sentence Embedding\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # 3. Transformer Positional Embedding：\n",
    "        seq_length = input_ids.size(1)  # 文章の長さ\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        # 3つの埋め込みテンソルを足し合わせる [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        # LayerNormalizationとDropoutを実行\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attentionモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        \n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_head) \n",
    "        self.all_head_size = self.num_attention_heads * self_attention_head_size\n",
    "        \n",
    "        # Self-Attentionの特徴量の全結合層\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        \n",
    "        # 入力を全結合層で特徴量変換\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        # multi-head Attention用にテンソルの形を変換\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        # 特徴量同士で積を取りAttention scoreとして求める\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / np.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # マスクがある部分にマスクをかける\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        \n",
    "        # Attentionを正規化\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(atteniton_probs)\n",
    "        \n",
    "        # Attention_mapの積を取る\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Layerモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        \n",
    "        # Self-Attentioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
